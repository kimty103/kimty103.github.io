---
layout: post
title: '[리뷰] ReFT: Representation Finetuning for Language Models '
---




<br>
*본 논문 리뷰은 CHATGPT로 작성되었으며 추후 자세한 내용은 수정될 예정입니다!!!*
<br>

원 논문 링크 : https://arxiv.org/pdf/2404.03592.pdf<br>
논문 깃헙 링크 : https://github.com/stanfordnlp/pyreft<br>

---
이 논문에서는 대형 언어 모델의 효율적인 파인 튜닝을 위한 새로운 접근법인 ReFT(Representation Finetuning) 방법을 제안합니다. 기존의 파라미터 효율적 파인 튜닝(PEFT) 방법은 소수의 가중치만을 업데이트하는데, 이러한 방법들은 제한적인 학습 용량 때문에 종종 최적의 성능을 발휘하지 못하는 문제가 있습니다.

### 논문 개요
언어 모델의 성능을 높이기 위해 기존에는 주로 모델 전체 또는 일부 파라미터를 업데이트하는 방식이 사용되었습니다. 그러나 이러한 방법들은 학습에 많은 자원을 요구하며, 때로는 효율성이 떨어질 수 있습니다. ReFT는 이러한 문제를 해결하기 위해 제안된 방법으로, 모델의 대부분을 동결시키고 오직 중요한 표현(representations)만을 수정하는 방식으로 파인 튜닝을 진행합니다.

### ReFT의 주요 특징
ReFT는 주로 은닉층(hidden layers)의 출력에 선형 변환을 적용하여, 특정 태스크에 최적화된 특성을 학습할 수 있도록 합니다. 이러한 접근 방식은 기존의 파인 튜닝 방식보다 훨씬 적은 파라미터를 수정함으로써, 계산 비용을 크게 줄이면서도 성능을 유지하거나 향상시킬 수 있습니다.

### LoReFT 방법
LoReFT(Low-rank Linear Subspace ReFT)는 ReFT의 한 형태로, 특히 낮은 순위의 선형 부공간을 이용하여 효율성을 극대화합니다. 이 방법은 기존 파인 튜닝 방법들과 비교했을 때, 훨씬 적은 수의 파라미터로 유사하거나 더 나은 성능을 달성할 수 있음을 보여줍니다.

### 성능 평가
이 논문에서는 여러 벤치마크 데이터셋을 사용하여 ReFT의 성능을 평가합니다. GLUE 벤치마크, Alpaca-Eval v1.0 등의 NLP 테스크에서 기존 방법들과 비교하여 ReFT가 얼마나 효과적인지를 보여주는 결과가 제시됩니다. 평가 결과, ReFT는 다수의 태스크에서 우수한 성능을 나타내며, 특히 자원 제약이 큰 환경에서의 잠재적인 이점을 강조합니다.

### 향후 전망
ReFT는 언어 모델의 파인 튜닝 방식을 효율적으로 개선할 수 있는 유망한 기술로 평가받고 있습니다. 논문은 이 기술이 언어 모델을 더 넓은 범위의 응용 분야에 적용할 수 있도록 하는 도구가 될 것이라고 전망합니다.